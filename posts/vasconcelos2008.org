#+TITLE: Notes: Space cost analysis using sized types
#+AUTHOR: Hector Suzanne
#+date:<2020-04-06 Mon>
* Introduction
** Programmation embarquée
On veut pouvoir utiliser des langages de haut niveaux dans les systèmes
embarqués pour des raison évidentes de productivité par rapport à C ou
l'assembleur. Mais cela vient avec une perte de prédictabilité quand à la
correction et la consommation de ressources des programmes. Les tests et le
profilage ne suffisent pas à /garantir/ le bon comportement des programmes de
haut niveaux dans les contextes embarqués: on y cherche des
performances prévisible et garanties plutôt meilleures à tout prix.

On distingue programmation /embarquée/, dans laquelle le programme doit jouer avec
des ressources limités génériques, et la programmation /en temps réel/, dans laquelle le
temps de réponse est la quantité bornée. Dans les systèmes embarqués
traditionnels, on garantie la terminaison et la correction en excluant les
constructions syntaxique les plus dangereuses (la récursion, l'allocation
dynamique, les boucles sans bornes sup triviales...). Mais le développement de
la programmation de haut niveau (java, python) sur micro-controlleurs et la
complexification des besoins en embarqué (IOT, exécution par un tierce partie)
ont créés de nouveaux besoins. (cf Stankovic 1988)
** Langages fonctionnels
Les langages fonctionnels permettent de garantir statiquement de bons invariants
/logiques/ de programmes, mais pas de bons invariants /de ressources/. Pourquoi:
- fonctions d'ordre supérieur :: Le flux de contrôle du programme est dynamique,
  et donc plus dur à évaluer. En C, le graphe de flux de contrôle est genérable
  statiquement.
- Copy-on-write data structures :: Même dans les cas simple d'un pipeline de
  transformation, on a pas d'assurance que l'implémentation va générer un
  programme travaillant en-place sur les structures déjà allouées.
- le ramasse-miette :: peut échouer à libérer la mémoire non-accessible dès
  qu'elle peut l'être, voir peut échouer à la libérer tout court. Ils
  obscurcissent la gestion mémoire.
- sémantiques non-strictes :: le coût d'un calcul ne dépend pas que de ça
  définition, mais aussi du contexte d'évaluation. Par exemple:
  =let null = fun [] -> true | _::_ -> false= n'a pas besoin d'évaluer
  entièrement son argument. Il est plus complexe de prendre en compte ces
  interaction appelant-appelé. Aussi, les thunks peuvent maintenir en vie des
  ressources, et donc peuvent causer des fuites mémoires.
- Optimisations :: On veut garantir les consommation du programme à l'exécution
  à partir du code source. Entre les deux se tient un compilateur-optimisateur,
  que l'on doit prendre en compte. Il faut adapter les passes d'optimisations.
** Contribution
On cherche à étendre l'application des langages fonctionnels à la programmation
critique et embarquée en créant une analyse automatique des consommations en
temps et mémoire des langages fonctionnels. Il s'agit d'une analyse statique
modulaire dirigée par les types pour prédire les bornes sup des consommations
mémoire. Les bornes sont exprimés par des compositions de fonctions familières
à croissance connues, en fonction non pas des valeurs des arguments, mais à partir
de grandeurs abstraites pertinentes extraites de leurs type.

On introduit un langage sur lequel porte l'analyse: /Hume/. Il est fonctionnel
strict, avec types et fonctions récursives, mais son cœur est d'ordre 1
uniquement. Il vient avec une machine abstraite à région mémoires et
ramasse-miette sans copie qui sert de modèle de coût pour ce langage. Cette
machine est basée sur SECD et est étendue pour supporter des optimisations
mémoires tractables pour l'analyse. Contrairement aux travaux précédents, on
supporte les types et tailles définis par l'utilisateur.

Notre contribution est limité de la manière suivante:
- On se restreint à /Hume core/, une simplification de Hume de premier ordre.
  Certaine constructions de Hume sont néanmoins admissibles dans Hume core.
- On impose des contraintes linéaires entre les coût/les tailles. Cela permet de
  garantir une analyse automatique par des technique d'interprétation abstraite
  (accélération de point fixe dans le domaine abstrait des polyèdres).
- On ne supporte pas les calculs de coût en temps. Il sera simple d'ajouter un
  registre abstrait à la machine, incrémenté lors de l'exécution et de
  l'analyser. Mais l'hypothèse d'uniformité en coût d'une transition de la
  machine peut ne pas tenir dans les systèmes réels. La machine abstraite de
  Hume garantie qu'une transition a toujours une consommation bornée, mais cette
  borne cache de nombreux détails d'implémentation. [NDLR on pense
  immédiatement aux lancements du GC]
** Exemple d'analyse
On considère le filtre simple, qui à un flux de flottant =x0,x1,...= renvoie un
flux =y0,y1,...= caractérisé par =yi = ∑(1≤k≤n) wk*x(i-k)=. Dans une
implémentation naïve en Haskell, il n'existe aucune garantie de libération des
éléments du premier flux quand ils ne sont plus nécessaires. Le ramasse miette
ne garantie pas non plus une borne précise de consommation mémoire pour cette
fonction.

En hume, on implémente un acteur synchrone qui génère le nouveau flux. La
dépendance entre (pre xs) et (post xs) est rendu explicite par la clause =wire=.
L'analyse statique détermine automatiquement que la liste qu'ils partagent un
invariant du programme.

#+BEGIN_SRC haskell
type Float = float 32 -- 32-bit floating point numbers
box fir
 in (x::Float, xs::[Float])
 out (y::Float, xs’::[Float])
match
 (x, xs) -> (dotp [0.5,2,0.5] xs, x:init xs)
wire fir.xs fir.xs’ initially [0,0,0]
#+END_SRC

Si on observe le type complet de =init=
=init : ∀n,m,s,h. < ∀a. List<n> a --(s,h)--> List<m> a | n=1+m, 0≤m, s≤6n-3,
h=3n-2>=
On remarque:
- la présence de paramètres de tailles dans les types =List<k>= garantissant que
  l'argument est non-vide (=0≤m=) et que la sortie contient un élément en plus (=n=m+1=).
- le type contient des informations de coût en pile et tas (=s= et =h=) et des
  bornes sur l'empreinte sur la pile et l'occupation mémoire. Ces propriétés
  ne sont pas purement dénotationnelle, mais demandent de fixer un modèle
  d'exécution (ici une machine virtuelle basée sur SECD).
- Le type est polymorphique en ses taille, et indique donc que l'implémentation
  est de consommation uniforme. Le polymorphisme permet d'avoir une analyse
  modulaire. On doit faire attention: les optimisations (comme la fusion)
  peuvent faire mentir le type obtenu (en fusionnant les créateurs et les
  consommateurs de structure par exemple).

  On extrait de l'analyse de type un système d'inéquations linéaires qui est
  automatiquement calculé par approximation de point fixe sur un treilli, comme
  en interprétation abstraite. On obtient au final:

  =fir.xs : Size ∈ [3,3], Stack ∈ [1,19], Heap ∈ [16,28]=
***
* Analyse de programme
** Objectif
Etendre le typage d'un langage pour y annoter des propriétés
dynamiques sur les valeurs, les calculs. Raisonner ensuite avec les outils de la
théorie des types. On raisonne uniquement sur des structures définies
inductivement, donc toujours modulo des equivalences [NDRL ie. dans des
groupoïdes].
** Premier exemple
On prend par exemple l'extension d'un λ-calcul simplement typé avec exceptions,
et un système d'effet modélisant ces exceptions: On a =Γ ⊢ e : t & φ=, avec φ un
ensemble d'exception, défini par =φ = ε | {e} | φ ∪ φ=. On implémente
inductivement l'égalité, l'appartenance, l'inclusion,... l'annotation des types
des fonctions avec l'ensemble des exceptions qu'elle peut lancer induit une une
relation de sous-typage simple sur les types annotées: On a

=a -(φ)-> b ≺ c -(ψ)->d iff. c ≺ a ∧ b ≺ d ∧ φ ⊂ ψ=

Et =≺= triviale autrement. Ce sous-typage est "shape-conformant", ce qui
simplifie l'inférence de type. Mais une relation de sous-typage plus complexe
peut améliorer la précision de l'analyse, en permettant de retarder
l'affaiblissement des effets (=φ⊂ψ=) jusqu'au point d'appel de la fonction analysée.
** Polymorphisme d'effets
On ajoute à notre système de type un =let= polymorphique, où l'on peut
quantifier sur les effets et les types. C'est essentiel pour une analyse fine
des paradigmes fonctionnels classique (plis, filtrage, nappage,...). On peut
alors typer, par exemple, la composition de fonction:

=compose : ∀a,b,c. ∀φ,ψ. (a -(φ)-> b) -> (b -(ψ)-> c) -> (a -(φ∪ψ)-> c)=

Pour notre système simple de lancement d'exception, le traitement du
polymorphisme est insidieusement simple: En général, il faut restreindre les
généralisation de type -- c'est à dire la liaison des variables libres sous un ∀
pour les domaines des formes =let=. Sinon, l'inférence est trop complexe
** Inférence
Le système d'effet exige de pouvoir "deviner" le type annoté d'expressions
quelconque. Il nous faut donc un algorithme d'inférence de type et d'effet. On
procède par /normalisation de preuves/: On restreint l'usage des règles de
typage non-structurelles à certains noeuds syntaxiques où l'on sait inférer les
types. Ici, on fait du /let-bound polymorphism/: les valeurs liées par des =let=
sont quantifiées au maximum sur leurs types et effets à la déclaration, puis
spécialisées au site d'utilisation de l'identifieur lié.

Avec notre relation de sous-effets moins forte que le sous-typage, ou peut
adapter l'algorithme /W/ de Damas. On restreint les annotations d'effets sur les
flèches à des variables fraiches uniquement, et on maintient séparément un
ensemble de contraintes de capacité =φ ⊂ ψ=. Alors, l'unification des types et
d'ordre 1, pour lequel on peut utiliser W. Sans séparer les contraintes, c'est
un problème d'unification modulo théorie des ensembles finis. Les contraintes
d'inclusion des effets sont résolues par un solveur à part.

Etendre cet approche avec une relation de sous-typage exigent "généralement"
(sic.) d'ajouter des contraintes d'inégalité de types afin d'inférer un typage
initial (au sens catégorique). La complétude de l'inférence de type dans ce cas
reste un problème ouvert, mais des algorithmes existent. Pour notre relation de
sous-typage /shape-conformant/, un algorithme en deux temps permet d'obtenir un
type pour tout terme, mais pas forcément son type initial. On procède en deux
temps:
- Inférence des types (/W/) et collecte des contraintes =φ ⊂ ψ=,
- Résolution des contraintes et mise-à-jour des types.

** Interprétation abstraite
/NDLR. C'est très classique, on mentionne vite fait les concepts déjà vu en TAS./
/On cite beaucoup le travail de Cousot&Cousot/
*** Concepts classiques
- Domaine abstrait en concret
- Structure de treillis sur ces domaines
- Connexion de Galois, et que faire quand il n'y en a pas
- Approximation de points fixes
- Opérateurs d'élargissement
- Treillis des intervalles
- Treillis des polyèdres convexes
*** Élargissement avec carburant
On commence l'itération de point fixe avec un nombre fini de carburant. Quand
l'usage de l'élargissement causerais une perte de précision, on consomme un
carburant et on fait une itération simple. Quand on a plus de carburant, on
élargi même si on perd de la précision. Cela est utile pour la convergence des
polyèdres.
Voir /Bagnara et al., Generation of ba- sic semi-algebraic invariants using
convex polyhedra/ pour plus d'information au besoin
* Analyse statique de consommation de ressource
** Analyse de complexité automatique
*** Préhistoire et système METRIC
On cherche a simuler/aider/automatiser les analyses manuelles de complexité
temporelle asymptotique de programmes. On procède comme un humain:
- Trouver une relation de récurrence sur le programme;
- En extraire une récurrence sur la compléxité,
- Avec une métrique d'entrée pertinente;
- Résoudre la relation, si possible.

Le premier de ces système, /METRIC/ (1975) suit de près les travaux séminaux de
Knuth (1973). On passe au relation entière en donnant un coût à chaque
primitive. Le résultat idéal est une formule close portant sur le coût de ces
primitive et soit la longueur en tant que liste des arguments, ou leur taille en
temps de S-expression.

Les métriques étudiés doivent être cumulatives et analytiques, donc on ne peut
pas mesurer la taille maximale de la pile. On peut néanmoins compter le nombre
de cellules /CONS/ allouées avec une métrique ou /CONS/ est de coût 1 et le
reste gratuit. Les types utilisateurs de sont analysés que comme S-expressions.
Enfin, l'hypothèse d'accumulation du temps n'est pas valide dans les sémantiques
non-strictes. Ce sera une épine récurrente.
*** Le système ACE de Métayer (1988)
Une nouvelle approche, basée sur la réécriture équationnelle. Le langage ciblé
est fonctionnel, applicatif, à base de combinateur, et est donc bien adapté au
problème. ACE produit une analyse au pire cas et asymptotique. Il procède par
réécriture selon l'algèbre applicative et le /principe d'induction récursive/ de
McCarthy : "Deux fonctions satisfaisant la même relatinon de récurrence sont
égale sur le domaine défini par point fixe par le bas de la relation". Le
système de réécriture contient plus de 1000 règles entrées manuellement, sans
étude (encore moins automatique) de leurs cohérence, ce qui aurait demandé bien
trop de travail (on est en 1988 tout de même). L'analyse de dit pas si le terme
asymptotique peut être dominé par les premiers termes, et ne supporte dans
l'analyse de la consommation mémoire.
*** Ensuite
- Rosendahl (1989) :: utilise l'interprétation abstraite pour définir une
  transformation de programme permettant d'associer, pour un préfixe d'entrée,
  une borne supérieure de la consommation de temps.
- Liu&Gomez (1998) :: ont une approche semblable à du profilage: on peut exécuter
  symboliquement le programme transformé pour obtenir des relations de récurrence
  sur le temps d'exécution à partir d'entrée partielle. On n'obtient pas de
  forme closes de ces relations.
- Walder (1988) :: brise le plafond de verre en proposant une analyse
  asymptomatique, et modulaire en présence de sémantique non-stricte, utilisant
  des /transformeurs de projection/ décrivant la "paresse" des fonctions
  analysées. Il n'est pas allé jusqu'à un algorithme d'analyse.
- Sands (1990) :: a créé plusieurs théories pour l'analyse des programmes
  fonctionnels avec fonctions d'ordre supérieur et évaluation paresseuse. Il
  peut ainsi obtenir des bornes [temps nécéssaire, temps suffisant] pour les
  fonctions paresseuses.
*** Conclusion
Ces formalismes ont pour but d'assister l'analyse asymptotique manuelle
d'algorithmes, et donc ne considèrent pas:
- L'automatisation des techniques d'analyse
- Les coûts d'exécution réels des programmes: On compte de nombre d'appels
  récursifs des fonctions.
** Système de type et d'effets pour le WCET
*** Travaux de Dornic & al
 Dornic&al. (1992) propose un "système de temps" pour un langage d'ordre
  supérieur à sémantique call-by-value. Une version spécialisé d'un système
  permettant de raisonner statiquement sur une classe de propriétés
  /intentionnelles/ des programmes à l'exécution. Les jugements de typages sont
  annotés d'un coût arithmétique entier. Les flèches sont annotés d'un coût
  latent de leur exécution.

  Le système devient intéressant quand on peut quantifier sur les coûts latent,
  ce qui permet de typer le coût les fonctions d'ordre supérieur en fonction du
  coût latent de leur argument.

  Mais de nombreuses limites:
  + Les fonctions récursives sont annotés avec un temps =long=, car pas de
    raisonnement sur la taille des arguments.
  + Pas de sous-typage des effets. Il est donc impossible de typer les jointures
    de deux calculs au coût différents, comme les deux branches d'une conditionnelle.
  + Pas d'inférence. Le problème sera résolu plus tard (1994)
*** Extensions
ce système a été étendu par Reistad & Gifford avec des annotation pour les
tailles des entiers naturels, des listes et des vecteurs. Ces annotations sont
des bornes sup statiques des tailles dynamiques des valeurs. Ils reconstruisent
les types et effets dans un système d'effets algébrique. Par exemple:

=succ : ∀n. Nat(n) -(1)-> >Nat(n+1)=
=map : ∀a,b,c,l. (a -(c)-> b) × List(a,n) -(k0+l*(k1+c))-> List(b,n)=


On remarque le polymorphisme de taille sur les arguments, qui permet de donner
un coût aux fonctions d'ordre supérieur usuelles. Comme le système de Dornic &
al., on ne gère pas la récursion, donc les types de ces schéma de récursions
primitif doivent être donnés.

=sub : ∀n,m. Nat(n) × Nat(m) -(c)-> Nat(n)= =twice succ : Nat long -(7)-> Nat
long=

On doit surestimer les tailles des résultats des fonctions
non-croissantes: on a =Nat(n)= en résultat de =seb=, alors que =Nat(n-m)= est
plus précis (et souhaitable). Enfin, le manque de polymorphisme cause des
surestimations de tailles. Dans le dernier exemple, on ne peux par obtenir le
typage souhaitable =Nat(n) -(7)-> Nat(n+2)=, par il faudra =succ= avec deux
types différents.

Une solution, de Loild (1998), consiste à étendre le système avec les types
intersections. Cela permet de spécialiser les fonctions polymorphiques sur
plusieurs arguments en parallèle, et donc de typer la double application de
=succ= de la forme =Nat(n) --> Nat(n+1) --> Nat(n+2)=

Finalement, Vasconcelos & Hammond (2004) ont étendu cette technique aux
définitions récurrentes, laissant le soin à l'utilisateur ou à un système
d'algèbre automatisé de clore les relations de récurrences. Les problèmes de
METRIC apparaissent aussi ici. On note aussi que l'approche se casse les dents
sur les algorithmes diviser-pour-régner tels que quicksort.

** Sized Types
*** Hughes, Pareto, Sabry (1996-2000)
Il existe des systèmes de types pour la déduction de propriétés de tailles
uniquement, pour les preuves de terminaisons, ou les optimisations. Les autheurs
présentent en 1996 un système de type étendu aux tailles pour prouver la
terminaison des programmes embarqués, et la propriétés /co/-respondante, la
productivité des programmes co-récursifs (comme les streams).

Les types des constructeurs sont annotés avec des bornes sup des tailles des
données construites pour les types récursifs, et des bornes sup pour les types
co-récursifs. Ces bornes sont limités à l'arithmétique de Presburger pour la
décidabilité, donc pas de multiplication native.

#+BEGIN_SRC
zero : Nat_{1}
succ : ∀i. Nat_{i} -> Nat_{i+1}

mk_stream : ∀i. ∀a. a -> Stream^{i} a -> Stream^{i+1} a
#+END_SRC

La relation d'ordre sur les tailles induit un sous-typage structurel sur les
sized types. On note ω la taille arbitrairement grande. Donc, =∀i. Nat_{i} ⊂
Nat_{ω}= On peut alors typer les expressions du genre

#+BEGIN_SRC
if cond then (??? : List_{i} a) else (??? : List_{j} a) : List_{k} a
#+END_SRC

en sur-approximant la taille de la branche la plus petite. On fini avec =k=max i
j= sans pour autant avoir l'opérateur =max= dans le système de type. La régle de
la récurrence (nouvelle) permet la récursion primitive sur les types à taille,
garantissant la terminaison des fonctions récursives et la productivité des
fonctions co-récursives.

On encode naturellement les récursions primitives sur les entiers et les
listes, et on peut aménager un polymorphisme de taille pour gérer les arguments
accumulateur des fonctions taill-call récursives comme =reverse=. Mais il faut
alors abandonner le polymorphisme classique pour garder la décidabilité de
l'analyse.

Les schéma de récursions non-linéaires ne sont pas aussi bien gérés. Prenons
l'exemple de =quicksort=: pour la fonction auxiliaire =pivot : ∀t. t -> list t
-> list t * list t=, la meilleure approximation est :

=pivot : ∀t. ∀i. t -> list_{i} t -> list_{i} t * list_{i} t=

mais la "bonne" approximation, qui permet de garantir la spécification de
=quicksort= serait:

=pivot : ∀t. ∀i,j. t -> list_{i+j} t -> list_{i} t * list_{j} t=,

mais on ne peut pas induire sur les sommes !

*** /Embedded ML/ : Hughes & Pareto (1999)

Extension de leurs système à un langage de programmation à la ML, avec
sémantique opérationelle à petit-pas basée sur la SECD, avec analyse des coûts
de pile et tas. On ajoute une primitive d'allocation de mémoire =letregion ρ#e
in e= où =ρ#e= est une nouvelle région mémoire de capacité dynamique.

Ce n'est hélas pas suffisant pour l'espace mémoire d'instant de système
synchrone, car la récursion peut créer des piles de régions. Le système est donc
augmenté d'un /region resetting/. Aussi, on ne peut pas implémenter les
structures infinies commes les streams.

*** Chin, Khoo (2001-2006)

Chin&Khoo ont un système avec inférence, basé sur l'arithmétique de Presburger,
qui est décidable (en passant par Omega). Il gèrent la récurrence avec une
opération de /fermeture transitive des contraintes linéaires/ permettant de
"fermer la boucle": passer d'une étape de récurrence au calcul complet.

/Exemple de résultat: =append=/
#+BEGIN_SRC
append : list_{m} t -> list_{n} t -> list{l} t
avec:
  - m ≥ 0, n ≥ 0, l = m + n
  - m > m' ≥ 0, n' = n
#+END_SRC
=n'= et =m'= sont les tailles à l'étape de récursion suivante. (comme une sorte
de post/pre)

On s'intéresse à des propriétés de /sécurité/, pas de /liveliness/ ou de
/productivité/ comme chez Hughes & Pareto. Chaque type possède sa propre notion
de taille: p.ex les booléens sont de taille 0 ou 1. On peut alors typer =null=
sur les listes de la manière suivante:

#+BEGIN_SRC
null : List_{n} a -> bool_{c} s.t. (n = 0 ∧ c = 1) ∨ (n > 0 ∧ c = 0)
#+END_SRC

Ca ressemble à ce qu'on peut faire avec des GADT, il faudra regarder ce lien de
plus près. On pourrait faire une équivalence entre les variables fantôme des
GADT (empty | nonempty) et des invariants de tailles (n = 0 | n > 0). On ferra
attention à ne pas oublier qu'on travaille à l'ordre supérieur. La discipline de
typage de Chin & Khoo n'a pas de preuve valide pour les types d'ordre
supérieurs. Leur preuve implique l'existence de contraintes décrivant exactement
la taille d'une valeur annoté d'un certain type. Ces contraintes n'existent pas
à l'ordre supérieure.

** Types Dépendants

*** /Dependent ML/

Créé en 1999, /dependent ML/ (DML) est une extension d'OCaml avec types
dépendants, mais relativement conservative. On y maintient la décidabilité du
typage et on s'y efforce de maintenir les annotations des types dépendants au
minimum. DML sépare les valeurs classique d'OCaml des /indices/ présents dans
les types, qui sont pris dans un domaine de contraintes décidables. On peut par
exemple utiliser des indices dans ℕ avec l'arithmétique de Presburger, qui sont
ensuite résolus par Omega: l'évaluation des indices est limitée à la
normalisation des contraintes.

Les types dépendants sont introduits par des raffinement de types afin de ne pas
avoir à changer le code non-dépendants. ={v:T} U= introduit le produit Π(v:T)U
et =[v:T | P]= introduit la somme Σ(v:T)P.

#+BEGIN_SRC
append <| {m:nat}{n:nat} 'a list(m) * 'a list(n) -> 'a list(m+n)
filter <| ('a -> bool) * {n:nat} 'a list(n) -> [m:nat| m<=n] 'a list(m)
#+END_SRC


- Avantage :: On peut définir nos propres notions de tailles, alors qu'avec les
  /Sized Types/ la notion est rigide.
- Inconvénient :: On ne résout par le problème de l'inférence, au contraire !
  On est passé au problème de l'inférence des types dépendants...

Grobauer a utilisé DML pour inférer des relations de récurrences sur les coûts
des calculs, mais ils faut encore les résoudre à la main.

*** LXres (Crary & Weirich)

/LXres/ est un langage de programmation avec type dépendants et
code-comme-preuve permettant d'exposer des "horloges virtuelles" au niveau des
types, et donc de mesurer les coûts liés à ces horloges. Ces estimations de
coûts sont des /fonctions primitives récursives/, un formalisme puissant pour le
problème en question. (En comparaison à, par exemple, l'arithmétique de
Presburger).

*** à la Epigram (Brady & Hammond)

Un langage dépendant avec un type =Size=: les valeurs =size v p : Size A P=
annotent les valeurs =v= indicés par une taille entière =n= et de type =A n=. La
preuve =p : P= témoigne alors d'une propriété de taille de =v=. On étend cette
technique aux fonctions d'ordre supérieur en associant des fonctions générant
les =Size= des arguments d'ordre supérieurs. Mais ces approximations de taille
pour les valeurs d'ordre supérieur ne sont pas inférées, et les obtenir
manuellement.

Aussi, le système infère des propriétés de taille, donc dénotationnelles, mais ne
considère pas l'obtention de propriétés intentionnelles. On ne peut pas
directement utiliser ces travaux pour obtenir des informations sur l'évaluation.

*** Cost Monad (Danielson)

#+BEGIN_SRC haskell
return :: a -> Thunk 0 a
bind :: Thunk n a -> (a -> Thunk m b) -> Thunk (n+m) b
tick :: Thunk n a -> Thunk (S n) a
#+END_SRC

Des monades avec un indice dépendant pour la taille, implémenté dans Agda. On
peut raisonner sur l'évaluation paresseuse en incluant directement les =Thunk=
dans des structures de données. Il faut par contre bien connaître Agda pour
obtenir des résultats : On est loin de l'analyse automatique de ressource. On
peut néanmoins étendre cette approche aux coût dans une machine virtuelle.
Encore une fois, par d'inférence.

** Analyse amortie

On passe tout le blabla usuel sur Tarjan. Merci à lui quand même. On note
l'existence de "Purely Functionnal Data Structures" de Okasaki, inspiré de sa
thèse de doctorat de 1996.

Hoffman & Jost ont présenté dans "Type-Based Amortised Heap-Space Analysis" une
méthode... d'analyse amortie d'estimation de l'usage du tas par des programmes
fonctionnels, induite par les types des valeurs allouées. On est dans le
contexte d'un langage fonctionnel du premier ordre, avec évaluation stricte. Le
langage libère explicitement la mémoire alloué dynamiquement par une indication
syntaxique sur les clauses =match=. [NDLR: ce n'est pas très restrictif en
pratique. Par exemple, GHC n'alloue que sur les =let= et ne libère que sur les
=case of=].

Dans cette analyse, les jugements et environnement des typages sont étendus par
des coefficients de poids dans le tas, et des potentiels. On n'infère pas des
tailles, mais la part de la consommation de tas des structures. Les estimations
de taille mémoire ne sont obtenus qu'avec les tailles inconnues des structures
en entrée et en sorties.

#+BEGIN_SRC
x : List(List(Bool, 1), 2), 3 ⊢ e : List(Bool, 4), 5
#+END_SRC
Signifie que =x= est une liste de liste de booléens, et que si =x= contient /n/
éléments de tailles /t_1,... t_n/, alors un tas de /3+2n+∑t_i/ suffit à évaluer
=e=, dont les /m/ éléments occuperont /4m+5/ cases du tas.

Cette analyse peut inférer les poids des types non-annotés. On peut donc la
qualifier d'automatique. Pour ce faire, elle associe au programme un système
d'équation linéaires dont les solutions sont les poids à inférer. On peut alors
utiliser un système de résolution linéaire efficace tierce pour obtenir les
annotations.

Limites: Les bornes de la taille du tas sont des expressions linéaires en la
tailles des entrées-sorties. Mais on peut quand même diviser pour régner en
découpant le potentiel. On manque aussi de polymorphisme pour l'analyse des
fonctions, qui prennent des poids fixes, qui doivent correspondre à tout leurs
site d'appels. Enfin, étendre l'analyse à la pile est non-trivial. Les bornes en
les tailles des structures sont linéaire, alors que l'occupation de la pile est
souvent sub-linéaire. Campbell, dans sa thèse de 2008, étend ce système à la
prise en compte de la /profondeur/ des structures considérées.
