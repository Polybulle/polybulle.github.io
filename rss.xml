<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
<channel>
  <title>Hector Suzanne</title>
  <description>Hector Suzanne</description>
  <link>hectorsuzanne.com</link>
  <lastBuildDate>Lun, 06 avr 2020 17:27:57 +0200</lastBuildDate>
<item>
  <title>Notes: Space cost analysis using sized types</title>
  <description><![CDATA[<div id="outline-container-orgacfe96d" class="outline-2">
<h2 id="orgacfe96d">Introduction</h2>
<div class="outline-text-2" id="text-orgacfe96d">
</div>
<div id="outline-container-org0dcacaf" class="outline-3">
<h3 id="org0dcacaf">Programmation embarquée</h3>
<div class="outline-text-3" id="text-org0dcacaf">
<p>
On veut pouvoir utiliser des langages de haut niveaux dans les systèmes
embarqués pour des raison évidentes de productivité par rapport à C ou
l'assembleur. Mais cela vient avec une perte de prédictabilité quand à la
correction et la consommation de ressources des programmes. Les tests et le
profilage ne suffisent pas à <i>garantir</i> le bon comportement des programmes de
haut niveaux dans les contextes embarqués: on y cherche des
performances prévisible et garanties plutôt meilleures à tout prix.
</p>

<p>
On distingue programmation <i>embarquée</i>, dans laquelle le programme doit jouer avec
des ressources limités génériques, et la programmation <i>en temps réel</i>, dans laquelle le
temps de réponse est la quantité bornée. Dans les systèmes embarqués
traditionnels, on garantie la terminaison et la correction en excluant les
constructions syntaxique les plus dangereuses (la récursion, l'allocation
dynamique, les boucles sans bornes sup triviales&#x2026;). Mais le développement de
la programmation de haut niveau (java, python) sur micro-controlleurs et la
complexification des besoins en embarqué (IOT, exécution par un tierce partie)
ont créés de nouveaux besoins. (cf Stankovic 1988)
</p>
</div>
</div>
<div id="outline-container-org30cd3ff" class="outline-3">
<h3 id="org30cd3ff">Langages fonctionnels</h3>
<div class="outline-text-3" id="text-org30cd3ff">
<p>
Les langages fonctionnels permettent de garantir statiquement de bons invariants
<i>logiques</i> de programmes, mais pas de bons invariants <i>de ressources</i>. Pourquoi:
</p>
<dl class="org-dl">
<dt>fonctions d'ordre supérieur</dt><dd>Le flux de contrôle du programme est dynamique,
et donc plus dur à évaluer. En C, le graphe de flux de contrôle est genérable
statiquement.</dd>
<dt>Copy-on-write data structures</dt><dd>Même dans les cas simple d'un pipeline de
transformation, on a pas d'assurance que l'implémentation va générer un
programme travaillant en-place sur les structures déjà allouées.</dd>
<dt>le ramasse-miette</dt><dd>peut échouer à libérer la mémoire non-accessible dès
qu'elle peut l'être, voir peut échouer à la libérer tout court. Ils
obscurcissent la gestion mémoire.</dd>
<dt>sémantiques non-strictes</dt><dd>le coût d'un calcul ne dépend pas que de ça
définition, mais aussi du contexte d'évaluation. Par exemple:
<code>let null = fun [] -&gt; true | _::_ -&gt; false</code> n'a pas besoin d'évaluer
entièrement son argument. Il est plus complexe de prendre en compte ces
interaction appelant-appelé. Aussi, les thunks peuvent maintenir en vie des
ressources, et donc peuvent causer des fuites mémoires.</dd>
<dt>Optimisations</dt><dd>On veut garantir les consommation du programme à l'exécution
à partir du code source. Entre les deux se tient un compilateur-optimisateur,
que l'on doit prendre en compte. Il faut adapter les passes d'optimisations.</dd>
</dl>
</div>
</div>
<div id="outline-container-org883e270" class="outline-3">
<h3 id="org883e270">Contribution</h3>
<div class="outline-text-3" id="text-org883e270">
<p>
On cherche à étendre l'application des langages fonctionnels à la programmation
critique et embarquée en créant une analyse automatique des consommations en
temps et mémoire des langages fonctionnels. Il s'agit d'une analyse statique
modulaire dirigée par les types pour prédire les bornes sup des consommations
mémoire. Les bornes sont exprimés par des compositions de fonctions familières
à croissance connues, en fonction non pas des valeurs des arguments, mais à partir
de grandeurs abstraites pertinentes extraites de leurs type.
</p>

<p>
On introduit un langage sur lequel porte l'analyse: <i>Hume</i>. Il est fonctionnel
strict, avec types et fonctions récursives, mais son cœur est d'ordre 1
uniquement. Il vient avec une machine abstraite à région mémoires et
ramasse-miette sans copie qui sert de modèle de coût pour ce langage. Cette
machine est basée sur SECD et est étendue pour supporter des optimisations
mémoires tractables pour l'analyse. Contrairement aux travaux précédents, on
supporte les types et tailles définis par l'utilisateur.
</p>

<p>
Notre contribution est limité de la manière suivante:
</p>
<ul class="org-ul">
<li>On se restreint à <i>Hume core</i>, une simplification de Hume de premier ordre.
Certaine constructions de Hume sont néanmoins admissibles dans Hume core.</li>
<li>On impose des contraintes linéaires entre les coût/les tailles. Cela permet de
garantir une analyse automatique par des technique d'interprétation abstraite
(accélération de point fixe dans le domaine abstrait des polyèdres).</li>
<li>On ne supporte pas les calculs de coût en temps. Il sera simple d'ajouter un
registre abstrait à la machine, incrémenté lors de l'exécution et de
l'analyser. Mais l'hypothèse d'uniformité en coût d'une transition de la
machine peut ne pas tenir dans les systèmes réels. La machine abstraite de
Hume garantie qu'une transition a toujours une consommation bornée, mais cette
borne cache de nombreux détails d'implémentation. [NDLR on pense
immédiatement aux lancements du GC]</li>
</ul>
</div>
</div>
<div id="outline-container-orgd8c00e2" class="outline-3">
<h3 id="orgd8c00e2">Exemple d'analyse</h3>
<div class="outline-text-3" id="text-orgd8c00e2">
<p>
On considère le filtre simple, qui à un flux de flottant <code>x0,x1,...</code> renvoie un
flux <code>y0,y1,...</code> caractérisé par <code>yi = ∑(1≤k≤n) wk*x(i-k)</code>. Dans une
implémentation naïve en Haskell, il n'existe aucune garantie de libération des
éléments du premier flux quand ils ne sont plus nécessaires. Le ramasse miette
ne garantie pas non plus une borne précise de consommation mémoire pour cette
fonction.
</p>

<p>
En hume, on implémente un acteur synchrone qui génère le nouveau flux. La
dépendance entre (pre xs) et (post xs) est rendu explicite par la clause <code>wire</code>.
L'analyse statique détermine automatiquement que la liste qu'ils partagent un
invariant du programme.
</p>

<div class="org-src-container">
<pre class="src src-haskell">type Float = float 32 -- 32-bit floating point numbers
box fir
 in (x::Float, xs::[Float])
 out (y::Float, xs’::[Float])
match
 (x, xs) -&gt; (dotp [0.5,2,0.5] xs, x:init xs)
wire fir.xs fir.xs’ initially [0,0,0]
</pre>
</div>

<p>
Si on observe le type complet de <code>init</code>
<code>init : ∀n,m,s,h. &lt; ∀a. List&lt;n&gt; a --(s,h)--&gt; List&lt;m&gt; a | n=1+m, 0≤m, s≤6n-3,
h=3n-2&gt;</code>
On remarque:
</p>
<ul class="org-ul">
<li>la présence de paramètres de tailles dans les types <code>List&lt;k&gt;</code> garantissant que
l'argument est non-vide (<code>0≤m</code>) et que la sortie contient un élément en plus (<code>n=m+1</code>).</li>
<li>le type contient des informations de coût en pile et tas (<code>s</code> et <code>h</code>) et des
bornes sur l'empreinte sur la pile et l'occupation mémoire. Ces propriétés
ne sont pas purement dénotationnelle, mais demandent de fixer un modèle
d'exécution (ici une machine virtuelle basée sur SECD).</li>
<li><p>
Le type est polymorphique en ses taille, et indique donc que l'implémentation
est de consommation uniforme. Le polymorphisme permet d'avoir une analyse
modulaire. On doit faire attention: les optimisations (comme la fusion)
peuvent faire mentir le type obtenu (en fusionnant les créateurs et les
consommateurs de structure par exemple).
</p>

<p>
On extrait de l'analyse de type un système d'inéquations linéaires qui est
automatiquement calculé par approximation de point fixe sur un treilli, comme
en interprétation abstraite. On obtient au final:
</p>

<p>
<code>fir.xs : Size ∈ [3,3], Stack ∈ [1,19], Heap ∈ [16,28]</code>
</p></li>
</ul>
<p>
<b>*</b>
</p>
</div>
</div>
</div>
<div id="outline-container-org4dbbf59" class="outline-2">
<h2 id="org4dbbf59">Analyse de programme</h2>
<div class="outline-text-2" id="text-org4dbbf59">
</div>
<div id="outline-container-org55a4e08" class="outline-3">
<h3 id="org55a4e08">Objectif</h3>
<div class="outline-text-3" id="text-org55a4e08">
<p>
Etendre le typage d'un langage pour y annoter des propriétés
dynamiques sur les valeurs, les calculs. Raisonner ensuite avec les outils de la
théorie des types. On raisonne uniquement sur des structures définies
inductivement, donc toujours modulo des equivalences [NDRL ie. dans des
groupoïdes].
</p>
</div>
</div>
<div id="outline-container-org3128f5b" class="outline-3">
<h3 id="org3128f5b">Premier exemple</h3>
<div class="outline-text-3" id="text-org3128f5b">
<p>
On prend par exemple l'extension d'un λ-calcul simplement typé avec exceptions,
et un système d'effet modélisant ces exceptions: On a <code>Γ ⊢ e : t &amp; φ</code>, avec φ un
ensemble d'exception, défini par <code>φ = ε | {e} | φ ∪ φ</code>. On implémente
inductivement l'égalité, l'appartenance, l'inclusion,&#x2026; l'annotation des types
des fonctions avec l'ensemble des exceptions qu'elle peut lancer induit une une
relation de sous-typage simple sur les types annotées: On a
</p>

<p>
<code>a -(φ)-&gt; b ≺ c -(ψ)-&gt;d iff. c ≺ a ∧ b ≺ d ∧ φ ⊂ ψ</code>
</p>

<p>
Et <code>≺</code> triviale autrement. Ce sous-typage est "shape-conformant", ce qui
simplifie l'inférence de type. Mais une relation de sous-typage plus complexe
peut améliorer la précision de l'analyse, en permettant de retarder
l'affaiblissement des effets (<code>φ⊂ψ</code>) jusqu'au point d'appel de la fonction analysée.
</p>
</div>
</div>
<div id="outline-container-org7180a6a" class="outline-3">
<h3 id="org7180a6a">Polymorphisme d'effets</h3>
<div class="outline-text-3" id="text-org7180a6a">
<p>
On ajoute à notre système de type un <code>let</code> polymorphique, où l'on peut
quantifier sur les effets et les types. C'est essentiel pour une analyse fine
des paradigmes fonctionnels classique (plis, filtrage, nappage,&#x2026;). On peut
alors typer, par exemple, la composition de fonction:
</p>

<p>
<code>compose : ∀a,b,c. ∀φ,ψ. (a -(φ)-&gt; b) -&gt; (b -(ψ)-&gt; c) -&gt; (a -(φ∪ψ)-&gt; c)</code>
</p>

<p>
Pour notre système simple de lancement d'exception, le traitement du
polymorphisme est insidieusement simple: En général, il faut restreindre les
généralisation de type &#x2013; c'est à dire la liaison des variables libres sous un ∀
pour les domaines des formes <code>let</code>. Sinon, l'inférence est trop complexe
</p>
</div>
</div>
<div id="outline-container-orga94bcd6" class="outline-3">
<h3 id="orga94bcd6">Inférence</h3>
<div class="outline-text-3" id="text-orga94bcd6">
<p>
Le système d'effet exige de pouvoir "deviner" le type annoté d'expressions
quelconque. Il nous faut donc un algorithme d'inférence de type et d'effet. On
procède par <i>normalisation de preuves</i>: On restreint l'usage des règles de
typage non-structurelles à certains noeuds syntaxiques où l'on sait inférer les
types. Ici, on fait du <i>let-bound polymorphism</i>: les valeurs liées par des <code>let</code>
sont quantifiées au maximum sur leurs types et effets à la déclaration, puis
spécialisées au site d'utilisation de l'identifieur lié.
</p>

<p>
Avec notre relation de sous-effets moins forte que le sous-typage, ou peut
adapter l'algorithme <i>W</i> de Damas. On restreint les annotations d'effets sur les
flèches à des variables fraiches uniquement, et on maintient séparément un
ensemble de contraintes de capacité <code>φ ⊂ ψ</code>. Alors, l'unification des types et
d'ordre 1, pour lequel on peut utiliser W. Sans séparer les contraintes, c'est
un problème d'unification modulo théorie des ensembles finis. Les contraintes
d'inclusion des effets sont résolues par un solveur à part.
</p>

<p>
Etendre cet approche avec une relation de sous-typage exigent "généralement"
(sic.) d'ajouter des contraintes d'inégalité de types afin d'inférer un typage
initial (au sens catégorique). La complétude de l'inférence de type dans ce cas
reste un problème ouvert, mais des algorithmes existent. Pour notre relation de
sous-typage <i>shape-conformant</i>, un algorithme en deux temps permet d'obtenir un
type pour tout terme, mais pas forcément son type initial. On procède en deux
temps:
</p>
<ul class="org-ul">
<li>Inférence des types (<i>W</i>) et collecte des contraintes <code>φ ⊂ ψ</code>,</li>
<li>Résolution des contraintes et mise-à-jour des types.</li>
</ul>
</div>
</div>

<div id="outline-container-org265f102" class="outline-3">
<h3 id="org265f102">Interprétation abstraite</h3>
<div class="outline-text-3" id="text-org265f102">
<p>
<i>NDLR. C'est très classique, on mentionne vite fait les concepts déjà vu en TAS.</i>
<i>On cite beaucoup le travail de Cousot&amp;Cousot</i>
</p>
</div>
<div id="outline-container-org5d94fbd" class="outline-4">
<h4 id="org5d94fbd">Concepts classiques</h4>
<div class="outline-text-4" id="text-org5d94fbd">
<ul class="org-ul">
<li>Domaine abstrait en concret</li>
<li>Structure de treillis sur ces domaines</li>
<li>Connexion de Galois, et que faire quand il n'y en a pas</li>
<li>Approximation de points fixes</li>
<li>Opérateurs d'élargissement</li>
<li>Treillis des intervalles</li>
<li>Treillis des polyèdres convexes</li>
</ul>
</div>
</div>
<div id="outline-container-org92e3723" class="outline-4">
<h4 id="org92e3723">Élargissement avec carburant</h4>
<div class="outline-text-4" id="text-org92e3723">
<p>
On commence l'itération de point fixe avec un nombre fini de carburant. Quand
l'usage de l'élargissement causerais une perte de précision, on consomme un
carburant et on fait une itération simple. Quand on a plus de carburant, on
élargi même si on perd de la précision. Cela est utile pour la convergence des
polyèdres.
Voir <i>Bagnara et al., Generation of ba- sic semi-algebraic invariants using
convex polyhedra</i> pour plus d'information au besoin
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orge20941a" class="outline-2">
<h2 id="orge20941a">Analyse statique de consommation de ressource</h2>
<div class="outline-text-2" id="text-orge20941a">
</div>
<div id="outline-container-org00eb091" class="outline-3">
<h3 id="org00eb091">Analyse de complexité automatique</h3>
<div class="outline-text-3" id="text-org00eb091">
</div>
<div id="outline-container-org4adf84d" class="outline-4">
<h4 id="org4adf84d">Préhistoire et système METRIC</h4>
<div class="outline-text-4" id="text-org4adf84d">
<p>
On cherche a simuler/aider/automatiser les analyses manuelles de complexité
temporelle asymptotique de programmes. On procède comme un humain:
</p>
<ul class="org-ul">
<li>Trouver une relation de récurrence sur le programme;</li>
<li>En extraire une récurrence sur la compléxité,</li>
<li>Avec une métrique d'entrée pertinente;</li>
<li>Résoudre la relation, si possible.</li>
</ul>

<p>
Le premier de ces système, <i>METRIC</i> (1975) suit de près les travaux séminaux de
Knuth (1973). On passe au relation entière en donnant un coût à chaque
primitive. Le résultat idéal est une formule close portant sur le coût de ces
primitive et soit la longueur en tant que liste des arguments, ou leur taille en
temps de S-expression.
</p>

<p>
Les métriques étudiés doivent être cumulatives et analytiques, donc on ne peut
pas mesurer la taille maximale de la pile. On peut néanmoins compter le nombre
de cellules <i>CONS</i> allouées avec une métrique ou <i>CONS</i> est de coût 1 et le
reste gratuit. Les types utilisateurs de sont analysés que comme S-expressions.
Enfin, l'hypothèse d'accumulation du temps n'est pas valide dans les sémantiques
non-strictes. Ce sera une épine récurrente.
</p>
</div>
</div>
<div id="outline-container-org1c40c58" class="outline-4">
<h4 id="org1c40c58">Le système ACE de Métayer (1988)</h4>
<div class="outline-text-4" id="text-org1c40c58">
<p>
Une nouvelle approche, basée sur la réécriture équationnelle. Le langage ciblé
est fonctionnel, applicatif, à base de combinateur, et est donc bien adapté au
problème. ACE produit une analyse au pire cas et asymptotique. Il procède par
réécriture selon l'algèbre applicative et le <i>principe d'induction récursive</i> de
McCarthy : "Deux fonctions satisfaisant la même relatinon de récurrence sont
égale sur le domaine défini par point fixe par le bas de la relation". Le
système de réécriture contient plus de 1000 règles entrées manuellement, sans
étude (encore moins automatique) de leurs cohérence, ce qui aurait demandé bien
trop de travail (on est en 1988 tout de même). L'analyse de dit pas si le terme
asymptotique peut être dominé par les premiers termes, et ne supporte dans
l'analyse de la consommation mémoire.
</p>
</div>
</div>
<div id="outline-container-orgdff7897" class="outline-4">
<h4 id="orgdff7897">Ensuite</h4>
<div class="outline-text-4" id="text-orgdff7897">
<dl class="org-dl">
<dt>Rosendahl (1989)</dt><dd>utilise l'interprétation abstraite pour définir une
transformation de programme permettant d'associer, pour un préfixe d'entrée,
une borne supérieure de la consommation de temps.</dd>
<dt>Liu&amp;Gomez (1998)</dt><dd>ont une approche semblable à du profilage: on peut exécuter
symboliquement le programme transformé pour obtenir des relations de récurrence
sur le temps d'exécution à partir d'entrée partielle. On n'obtient pas de
forme closes de ces relations.</dd>
<dt>Walder (1988)</dt><dd>brise le plafond de verre en proposant une analyse
asymptomatique, et modulaire en présence de sémantique non-stricte, utilisant
des <i>transformeurs de projection</i> décrivant la "paresse" des fonctions
analysées. Il n'est pas allé jusqu'à un algorithme d'analyse.</dd>
<dt>Sands (1990)</dt><dd>a créé plusieurs théories pour l'analyse des programmes
fonctionnels avec fonctions d'ordre supérieur et évaluation paresseuse. Il
peut ainsi obtenir des bornes [temps nécéssaire, temps suffisant] pour les
fonctions paresseuses.</dd>
</dl>
</div>
</div>
<div id="outline-container-org3faf30f" class="outline-4">
<h4 id="org3faf30f">Conclusion</h4>
<div class="outline-text-4" id="text-org3faf30f">
<p>
Ces formalismes ont pour but d'assister l'analyse asymptotique manuelle
d'algorithmes, et donc ne considèrent pas:
</p>
<ul class="org-ul">
<li>L'automatisation des techniques d'analyse</li>
<li>Les coûts d'exécution réels des programmes: On compte de nombre d'appels
récursifs des fonctions.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org4bdcf41" class="outline-3">
<h3 id="org4bdcf41">Système de type et d'effets pour le WCET</h3>
<div class="outline-text-3" id="text-org4bdcf41">
</div>
<div id="outline-container-org234ba35" class="outline-4">
<h4 id="org234ba35">Travaux de Dornic &amp; al</h4>
<div class="outline-text-4" id="text-org234ba35">
<p>
Dornic&amp;al. (1992) propose un "système de temps" pour un langage d'ordre
 supérieur à sémantique call-by-value. Une version spécialisé d'un système
 permettant de raisonner statiquement sur une classe de propriétés
 <i>intentionnelles</i> des programmes à l'exécution. Les jugements de typages sont
 annotés d'un coût arithmétique entier. Les flèches sont annotés d'un coût
 latent de leur exécution.
</p>

<p>
Le système devient intéressant quand on peut quantifier sur les coûts latent,
ce qui permet de typer le coût les fonctions d'ordre supérieur en fonction du
coût latent de leur argument.
</p>

<p>
Mais de nombreuses limites:
</p>
<ul class="org-ul">
<li>Les fonctions récursives sont annotés avec un temps <code>long</code>, car pas de
raisonnement sur la taille des arguments.</li>
<li>Pas de sous-typage des effets. Il est donc impossible de typer les jointures
de deux calculs au coût différents, comme les deux branches d'une conditionnelle.</li>
<li>Pas d'inférence. Le problème sera résolu plus tard (1994)</li>
</ul>
</div>
</div>
<div id="outline-container-org40f528c" class="outline-4">
<h4 id="org40f528c">Extensions</h4>
<div class="outline-text-4" id="text-org40f528c">
<p>
ce système a été étendu par Reistad &amp; Gifford avec des annotation pour les
tailles des entiers naturels, des listes et des vecteurs. Ces annotations sont
des bornes sup statiques des tailles dynamiques des valeurs. Ils reconstruisent
les types et effets dans un système d'effets algébrique. Par exemple:
</p>

<p>
<code>succ : ∀n. Nat(n) -(1)-&gt; &gt;Nat(n+1)</code>
<code>map : ∀a,b,c,l. (a -(c)-&gt; b) × List(a,n) -(k0+l*(k1+c))-&gt; List(b,n)</code>
</p>


<p>
On remarque le polymorphisme de taille sur les arguments, qui permet de donner
un coût aux fonctions d'ordre supérieur usuelles. Comme le système de Dornic &amp;
al., on ne gère pas la récursion, donc les types de ces schéma de récursions
primitif doivent être donnés.
</p>

<p>
<code>sub : ∀n,m. Nat(n) × Nat(m) -(c)-&gt; Nat(n)</code> <code>twice succ : Nat long -(7)-&gt; Nat
long</code>
</p>

<p>
On doit surestimer les tailles des résultats des fonctions
non-croissantes: on a <code>Nat(n)</code> en résultat de <code>seb</code>, alors que <code>Nat(n-m)</code> est
plus précis (et souhaitable). Enfin, le manque de polymorphisme cause des
surestimations de tailles. Dans le dernier exemple, on ne peux par obtenir le
typage souhaitable <code>Nat(n) -(7)-&gt; Nat(n+2)</code>, par il faudra <code>succ</code> avec deux
types différents.
</p>

<p>
Une solution, de Loild (1998), consiste à étendre le système avec les types
intersections. Cela permet de spécialiser les fonctions polymorphiques sur
plusieurs arguments en parallèle, et donc de typer la double application de
<code>succ</code> de la forme <code>Nat(n) --&gt; Nat(n+1) --&gt; Nat(n+2)</code>
</p>

<p>
Finalement, Vasconcelos &amp; Hammond (2004) ont étendu cette technique aux
définitions récurrentes, laissant le soin à l'utilisateur ou à un système
d'algèbre automatisé de clore les relations de récurrences. Les problèmes de
METRIC apparaissent aussi ici. On note aussi que l'approche se casse les dents
sur les algorithmes diviser-pour-régner tels que quicksort.
</p>
</div>
</div>
</div>

<div id="outline-container-org7bddc35" class="outline-3">
<h3 id="org7bddc35">Sized Types</h3>
<div class="outline-text-3" id="text-org7bddc35">
</div>
<div id="outline-container-org233091d" class="outline-4">
<h4 id="org233091d">Hughes, Pareto, Sabry (1996-2000)</h4>
<div class="outline-text-4" id="text-org233091d">
<p>
Il existe des systèmes de types pour la déduction de propriétés de tailles
uniquement, pour les preuves de terminaisons, ou les optimisations. Les autheurs
présentent en 1996 un système de type étendu aux tailles pour prouver la
terminaison des programmes embarqués, et la propriétés <i>co</i>-respondante, la
productivité des programmes co-récursifs (comme les streams).
</p>

<p>
Les types des constructeurs sont annotés avec des bornes sup des tailles des
données construites pour les types récursifs, et des bornes sup pour les types
co-récursifs. Ces bornes sont limités à l'arithmétique de Presburger pour la
décidabilité, donc pas de multiplication native.
</p>

<pre class="example">
zero : Nat_{1}
succ : ∀i. Nat_{i} -&gt; Nat_{i+1}

mk_stream : ∀i. ∀a. a -&gt; Stream^{i} a -&gt; Stream^{i+1} a
</pre>

<p>
La relation d'ordre sur les tailles induit un sous-typage structurel sur les
sized types. On note ω la taille arbitrairement grande. Donc, <code>∀i. Nat_{i} ⊂
Nat_{ω}</code> On peut alors typer les expressions du genre
</p>

<pre class="example">
if cond then (??? : List_{i} a) else (??? : List_{j} a) : List_{k} a
</pre>

<p>
en sur-approximant la taille de la branche la plus petite. On fini avec <code>k=max i
j</code> sans pour autant avoir l'opérateur <code>max</code> dans le système de type. La régle de
la récurrence (nouvelle) permet la récursion primitive sur les types à taille,
garantissant la terminaison des fonctions récursives et la productivité des
fonctions co-récursives.
</p>

<p>
On encode naturellement les récursions primitives sur les entiers et les
listes, et on peut aménager un polymorphisme de taille pour gérer les arguments
accumulateur des fonctions taill-call récursives comme <code>reverse</code>. Mais il faut
alors abandonner le polymorphisme classique pour garder la décidabilité de
l'analyse.
</p>

<p>
Les schéma de récursions non-linéaires ne sont pas aussi bien gérés. Prenons
l'exemple de <code>quicksort</code>: pour la fonction auxiliaire <code>pivot : ∀t. t -&gt; list t
-&gt; list t * list t</code>, la meilleure approximation est :
</p>

<p>
<code>pivot : ∀t. ∀i. t -&gt; list_{i} t -&gt; list_{i} t * list_{i} t</code>
</p>

<p>
mais la "bonne" approximation, qui permet de garantir la spécification de
<code>quicksort</code> serait:
</p>

<p>
<code>pivot : ∀t. ∀i,j. t -&gt; list_{i+j} t -&gt; list_{i} t * list_{j} t</code>,
</p>

<p>
mais on ne peut pas induire sur les sommes !
</p>
</div>
</div>

<div id="outline-container-org1e2cfd0" class="outline-4">
<h4 id="org1e2cfd0"><i>Embedded ML</i> : Hughes &amp; Pareto (1999)</h4>
<div class="outline-text-4" id="text-org1e2cfd0">
<p>
Extension de leurs système à un langage de programmation à la ML, avec
sémantique opérationelle à petit-pas basée sur la SECD, avec analyse des coûts
de pile et tas. On ajoute une primitive d'allocation de mémoire <code>letregion ρ#e
in e</code> où <code>ρ#e</code> est une nouvelle région mémoire de capacité dynamique.
</p>

<p>
Ce n'est hélas pas suffisant pour l'espace mémoire d'instant de système
synchrone, car la récursion peut créer des piles de régions. Le système est donc
augmenté d'un <i>region resetting</i>. Aussi, on ne peut pas implémenter les
structures infinies commes les streams.
</p>
</div>
</div>

<div id="outline-container-orgd1e5783" class="outline-4">
<h4 id="orgd1e5783">Chin, Khoo (2001-2006)</h4>
<div class="outline-text-4" id="text-orgd1e5783">
<p>
Chin&amp;Khoo ont un système avec inférence, basé sur l'arithmétique de Presburger,
qui est décidable (en passant par Omega). Il gèrent la récurrence avec une
opération de <i>fermeture transitive des contraintes linéaires</i> permettant de
"fermer la boucle": passer d'une étape de récurrence au calcul complet.
</p>

<p>
<i>Exemple de résultat: <code>append</code></i>
</p>
<pre class="example">
append : list_{m} t -&gt; list_{n} t -&gt; list{l} t
avec:
  - m ≥ 0, n ≥ 0, l = m + n
  - m &gt; m' ≥ 0, n' = n
</pre>
<p>
<code>n'</code> et <code>m'</code> sont les tailles à l'étape de récursion suivante. (comme une sorte
de post/pre)
</p>

<p>
On s'intéresse à des propriétés de <i>sécurité</i>, pas de <i>liveliness</i> ou de
<i>productivité</i> comme chez Hughes &amp; Pareto. Chaque type possède sa propre notion
de taille: p.ex les booléens sont de taille 0 ou 1. On peut alors typer <code>null</code>
sur les listes de la manière suivante:
</p>

<pre class="example">
null : List_{n} a -&gt; bool_{c} s.t. (n = 0 ∧ c = 1) ∨ (n &gt; 0 ∧ c = 0)
</pre>

<p>
Ca ressemble à ce qu'on peut faire avec des GADT, il faudra regarder ce lien de
plus près. On pourrait faire une équivalence entre les variables fantôme des
GADT (empty | nonempty) et des invariants de tailles (n = 0 | n &gt; 0). On ferra
attention à ne pas oublier qu'on travaille à l'ordre supérieur. La discipline de
typage de Chin &amp; Khoo n'a pas de preuve valide pour les types d'ordre
supérieurs. Leur preuve implique l'existence de contraintes décrivant exactement
la taille d'une valeur annoté d'un certain type. Ces contraintes n'existent pas
à l'ordre supérieure.
</p>
</div>
</div>
</div>

<div id="outline-container-orgd268c6f" class="outline-3">
<h3 id="orgd268c6f">Types Dépendants</h3>
<div class="outline-text-3" id="text-orgd268c6f">
</div>
<div id="outline-container-org6089895" class="outline-4">
<h4 id="org6089895"><i>Dependent ML</i></h4>
<div class="outline-text-4" id="text-org6089895">
<p>
Créé en 1999, <i>dependent ML</i> (DML) est une extension d'OCaml avec types
dépendants, mais relativement conservative. On y maintient la décidabilité du
typage et on s'y efforce de maintenir les annotations des types dépendants au
minimum. DML sépare les valeurs classique d'OCaml des <i>indices</i> présents dans
les types, qui sont pris dans un domaine de contraintes décidables. On peut par
exemple utiliser des indices dans ℕ avec l'arithmétique de Presburger, qui sont
ensuite résolus par Omega: l'évaluation des indices est limitée à la
normalisation des contraintes.
</p>

<p>
Les types dépendants sont introduits par des raffinement de types afin de ne pas
avoir à changer le code non-dépendants. <code>{v:T} U</code> introduit le produit Π(v:T)U
et <code>[v:T | P]</code> introduit la somme Σ(v:T)P.
</p>

<pre class="example">
append &lt;| {m:nat}{n:nat} 'a list(m) * 'a list(n) -&gt; 'a list(m+n)
filter &lt;| ('a -&gt; bool) * {n:nat} 'a list(n) -&gt; [m:nat| m&lt;=n] 'a list(m)
</pre>


<dl class="org-dl">
<dt>Avantage</dt><dd>On peut définir nos propres notions de tailles, alors qu'avec les
<i>Sized Types</i> la notion est rigide.</dd>
<dt>Inconvénient</dt><dd>On ne résout par le problème de l'inférence, au contraire !
On est passé au problème de l'inférence des types dépendants&#x2026;</dd>
</dl>

<p>
Grobauer a utilisé DML pour inférer des relations de récurrences sur les coûts
des calculs, mais ils faut encore les résoudre à la main.
</p>
</div>
</div>

<div id="outline-container-org3dc90d9" class="outline-4">
<h4 id="org3dc90d9">LXres (Crary &amp; Weirich)</h4>
<div class="outline-text-4" id="text-org3dc90d9">
<p>
<i>LXres</i> est un langage de programmation avec type dépendants et
code-comme-preuve permettant d'exposer des "horloges virtuelles" au niveau des
types, et donc de mesurer les coûts liés à ces horloges. Ces estimations de
coûts sont des <i>fonctions primitives récursives</i>, un formalisme puissant pour le
problème en question. (En comparaison à, par exemple, l'arithmétique de
Presburger).
</p>
</div>
</div>

<div id="outline-container-org899525d" class="outline-4">
<h4 id="org899525d">à la Epigram (Brady &amp; Hammond)</h4>
<div class="outline-text-4" id="text-org899525d">
<p>
Un langage dépendant avec un type <code>Size</code>: les valeurs <code>size v p : Size A P</code>
annotent les valeurs <code>v</code> indicés par une taille entière <code>n</code> et de type <code>A n</code>. La
preuve <code>p : P</code> témoigne alors d'une propriété de taille de <code>v</code>. On étend cette
technique aux fonctions d'ordre supérieur en associant des fonctions générant
les <code>Size</code> des arguments d'ordre supérieurs. Mais ces approximations de taille
pour les valeurs d'ordre supérieur ne sont pas inférées, et les obtenir
manuellement.
</p>

<p>
Aussi, le système infère des propriétés de taille, donc dénotationnelles, mais ne
considère pas l'obtention de propriétés intentionnelles. On ne peut pas
directement utiliser ces travaux pour obtenir des informations sur l'évaluation.
</p>
</div>
</div>

<div id="outline-container-org0e41143" class="outline-4">
<h4 id="org0e41143">Cost Monad (Danielson)</h4>
<div class="outline-text-4" id="text-org0e41143">
<div class="org-src-container">
<pre class="src src-haskell">return :: a -&gt; Thunk 0 a
bind :: Thunk n a -&gt; (a -&gt; Thunk m b) -&gt; Thunk (n+m) b
tick :: Thunk n a -&gt; Thunk (S n) a
</pre>
</div>

<p>
Des monades avec un indice dépendant pour la taille, implémenté dans Agda. On
peut raisonner sur l'évaluation paresseuse en incluant directement les <code>Thunk</code>
dans des structures de données. Il faut par contre bien connaître Agda pour
obtenir des résultats : On est loin de l'analyse automatique de ressource. On
peut néanmoins étendre cette approche aux coût dans une machine virtuelle.
Encore une fois, par d'inférence.
</p>
</div>
</div>
</div>

<div id="outline-container-org8fcbe17" class="outline-3">
<h3 id="org8fcbe17">Analyse amortie</h3>
<div class="outline-text-3" id="text-org8fcbe17">
<p>
On passe tout le blabla usuel sur Tarjan. Merci à lui quand même. On note
l'existence de "Purely Functionnal Data Structures" de Okasaki, inspiré de sa
thèse de doctorat de 1996.
</p>

<p>
Hoffman &amp; Jost ont présenté dans "Type-Based Amortised Heap-Space Analysis" une
méthode&#x2026; d'analyse amortie d'estimation de l'usage du tas par des programmes
fonctionnels, induite par les types des valeurs allouées. On est dans le
contexte d'un langage fonctionnel du premier ordre, avec évaluation stricte. Le
langage libère explicitement la mémoire alloué dynamiquement par une indication
syntaxique sur les clauses <code>match</code>. [NDLR: ce n'est pas très restrictif en
pratique. Par exemple, GHC n'alloue que sur les <code>let</code> et ne libère que sur les
=case of=].
</p>

<p>
Dans cette analyse, les jugements et environnement des typages sont étendus par
des coefficients de poids dans le tas, et des potentiels. On n'infère pas des
tailles, mais la part de la consommation de tas des structures. Les estimations
de taille mémoire ne sont obtenus qu'avec les tailles inconnues des structures
en entrée et en sorties.
</p>

<pre class="example">
x : List(List(Bool, 1), 2), 3 ⊢ e : List(Bool, 4), 5
</pre>
<p>
Signifie que <code>x</code> est une liste de liste de booléens, et que si <code>x</code> contient <i>n</i>
éléments de tailles <i>t<sub>1</sub>,&#x2026; t<sub>n</sub></i>, alors un tas de <i>3+2n+∑t<sub>i</sub></i> suffit à évaluer
<code>e</code>, dont les <i>m</i> éléments occuperont <i>4m+5</i> cases du tas.
</p>

<p>
Cette analyse peut inférer les poids des types non-annotés. On peut donc la
qualifier d'automatique. Pour ce faire, elle associe au programme un système
d'équation linéaires dont les solutions sont les poids à inférer. On peut alors
utiliser un système de résolution linéaire efficace tierce pour obtenir les
annotations.
</p>

<p>
Limites: Les bornes de la taille du tas sont des expressions linéaires en la
tailles des entrées-sorties. Mais on peut quand même diviser pour régner en
découpant le potentiel. On manque aussi de polymorphisme pour l'analyse des
fonctions, qui prennent des poids fixes, qui doivent correspondre à tout leurs
site d'appels. Enfin, étendre l'analyse à la pile est non-trivial. Les bornes en
les tailles des structures sont linéaire, alors que l'occupation de la pile est
souvent sub-linéaire. Campbell, dans sa thèse de 2008, étend ce système à la
prise en compte de la <i>profondeur</i> des structures considérées.
</p>
</div>
</div>
</div>
]]></description>
  <link>hectorsuzanne.comvasconcelos2008.html</link>
  <pubDate>Lun, 06 avr 2020 00:00:00 +0200</pubDate>
</item>
<item>
  <title>Notes de Lecture: Towards Automatic Resource Bound Analysis for OCaml</title>
  <description><![CDATA[<div id="outline-container-org3d1548d" class="outline-2">
<h2 id="org3d1548d">Introduction</h2>
<div class="outline-text-2" id="text-org3d1548d">
<p>
Contribution : une analyse statique de
ressources pour les programmes OCaml <i>fonctionnels</i>, <i>d'ordre supérieur</i>, <i>avec
polymorphisme et type utilisateurs</i>. On obtient des polynômes multivariables
décrivant l'usage de ressources tel que le WCET, l'occupation mémoire ou la
consommation d'énergie. On utilise <b>AARA : Automatic Amortized Resource
Analysis</b>, un npuveau système de type et une nouvelle sémantique de la machine
virtuelle <i>ZINC</i> d'OCaml.
On peut trouver des bornes asymptotique fortes pour des programmes
réels, comme les utilitaires standard sur les listes, l'algorithme de Dijkstra,
ou un client pour Amazon DynamoDB avec analyse statique du coût des accès au
nuage.
</p>

<div class="org-src-container">
<pre class="src src-ocaml">type ('a, 'b) ablist =
| Acons of 'a * ('a, 'b) list
| Bcons of 'b * ('a, 'b) list
| ABNil

let sort_the_as ablist = match ablist with
| Acons (a, tail) -&gt; Acons (quicksort a, sort_the_as tail)
| Bcons (b, tail) -&gt; Bcons (b          , sort_the_as tail)
| ABNil -&gt; ABNil

(*
  RAML detecte les bornes suivantes:
    N = nombre de noeuds Acons de `ablist`
    K = max des tailles des arguments 'a des Acons
    L = N+K = taille de `ablist`
    Cout de sort_the_as:
      13*K*N^2 + 22*K*N + 13*L + 15*N + 11

  On retrouve bien la borne asymptotique en O(K^2 * N)
*)
</pre>
</div>
</div>
</div>
<div id="outline-container-org1226049" class="outline-2">
<h2 id="org1226049">Survol</h2>
<div class="outline-text-2" id="text-org1226049">
<p>
On extrait un AST typé du compilateur OCaml, puis on infère de nouveaux types
contenant des information de potentiel pour chaque expression. D'une déclaration
<code>t : T</code>, on obtient une autre définition <code>t : (T'; &lt;qi...&gt;))</code> ou <code>T'</code> est proche
de <code>T</code> (on précise les applications partielle/totales des fonctions aux sites
d'appels), et les <code>qi</code> sont des coefficients permettant de définir un potentiel
φ(t) = Σ qi * pi(t). Les polynômes <code>pi</code> sont définis pour chaque constructeurs
du type <code>T</code>. La sémantique du langage hôte induit des contraintes linéaires
entre les potentiels. Elles sont résolues par un solveur de programmation
linéaire externe lors d'un parcours de l'AST, comme on infère les types dans un
programme en produisant et résolvant des contraintes d'unification.
</p>
</div>
</div>

<div id="outline-container-org1b3237d" class="outline-2">
<h2 id="org1b3237d">Analyse de la programmation fonctionnelle</h2>
<div class="outline-text-2" id="text-org1b3237d">
<p>
Il y a trois principale difficultés: Les fermeture/l'application partielle,
les arguments d'ordre supérieurs, et les effets de bords
</p>
</div>
<div id="outline-container-org3b53517" class="outline-4">
<h4 id="org3b53517">Fermetures et curryfication</h4>
<div class="outline-text-4" id="text-org3b53517">
<p>
On prend comme exemple <code>val append : 'a list -&gt; 'a list -&gt; 'a list</code> de la
bibliothèque standard. L'appel <code>let foo = append xs</code> termine en temps constant,
et ne fait que créer une fermeture. Par contre, l'appel suivant <code>foo ys</code> renvoie
la liste <code>xs ++ ys</code> en temps et espace O(xs). Donc, l'usage de ressource de
<code>foo</code> ne dépend pas de son argument <code>ys</code>, mais est linéaire en la taille de la
valeur capturée <code>xs</code>. Hélas, il n'est pas suffisant de dé-curryfier <code>append</code>
pour retomber sur un cas plus simple: Un appel à <code>append</code> nécessite d'allouer
une paire pour les deux arguments. Dans le cas normal (<code>append</code> est curryfiée),
on prduit une fermeture si l'application est partielle, ou on appelle <code>append</code>
sans allocation si les deux arguments sont fournis.
</p>

<p>
On résout ce problème en raffinant les types des fonctions sur leurs sites
d'appels. Les applications partielles peuvent être saturées par eta-expansion,
et on change le type de la fonction de <code>a1 -&gt; ... -&gt; an -&gt; t</code> en <code>[a1 ... an] -&gt;
t</code>, qui indique la saturation de l'appel. L'analyse rend compte de
l'usage correct des ressource pour le second type, mais pas pour le premier.
En effet, l'algorithme est bien plus complexe si les valeurs capturées
dans les fermetures interviennent dans le calcul de complexité. L'analyse est
guidée par ce principe: <i>La complexité d'un appel de fonction ne doit dépendre
que de la taille de ses arguments</i>
</p>

<p>
On reconnaît la possibilité d'étendre le système pour gérer ces cas, mais cela a
un coût important de compléxité de l'analyse. On pourrait par exemple étendre le
raffinement des types avec des types dépendants.
</p>
</div>
</div>
<div id="outline-container-org0c306f6" class="outline-4">
<h4 id="org0c306f6">Gestion de l'ordre supérieur</h4>
<div class="outline-text-4" id="text-org0c306f6">
<p>
Dans le cas plus simple où on se limite à des estimations linéaires du coût, il est
possible de calculer la consommation de ressource en présence d'arguments
d'ordre supérieur en se ramenant à l'ordre 1. Mais c'est seulement possible quand les arguments sont analysables, et on doit entrelacer le typage et la génération de
contraintes. On préfère séparer les deux processus.
</p>

<p>
On peut alors analyser des fonctions même en l'absence d'information sur les
arguments: Il suffit de supposer leurs coût négligeable. L'analyse n'est alors
pas veine: On peut déterminer le coût propre de fonctions comme <code>map</code> et <code>fold</code>,
et l'analyse sert de preuve de terminaison. On peut en effet dire qu'une
fonction ayant un coût défini termine si et seulement si les appels aux
arguments terminent tous.
</p>

<p>
Dans les cas où la terminaison de la fonction dépend de propriétés des arguments
d'ordre supérieur, on ne peut pas conclure dans le cas général. Mais on peut
reprendre l'analyse dans les sites d'appels où ces arguments sont connus !
exemple:
</p>

<div class="org-src-container">
<pre class="src src-ocaml">(* Impossible de conclure ici *)
let iter f x = let y = f x in if y = 0 then () else iter f y

(* mais ici on peut spécialiser pour une valeur présice de `f`*)
let foo x = iter (fun z -&gt; 0) x
</pre>
</div>
</div>
</div>

<div id="outline-container-orgf3a9b2c" class="outline-4">
<h4 id="orgf3a9b2c">Effets de bord</h4>
<div class="outline-text-4" id="text-orgf3a9b2c">
<p>
L'analyse vérifie que le coût du programme est indépendant des valeurs stockées
dans un état mutable quelconque. Dans le cas contraire, on peut peux pas
poursuivre. Il a été montré que l'analyse à base de polynômes peut être étendu
avec des potentiels pour les états mutables, mais le projet ne le fait pas, pour se
focaliser sur la contribution principale, l'analyse sur les programmes
fonctionnels.
L'analyse peut néanmoins interagir avec l'état mutable pour affiner ses
résultat. On peut par exemple, effectuer une analyse d'aliasing sur les
références à des fonctions: Si la référence en question ne peut prendre qu'un
nombre fini de valeurs (et qu'on peut le détecter,) alors on peut calculer un
coût au pire pour les appels à travers la référence.
Enfin, on peut étendre l'analyse aux programme lançant des exceptions en
annotant le type de l'exception et de ses arguments.
</p>
</div>
</div>
</div>
<div id="outline-container-orgbf865d1" class="outline-2">
<h2 id="orgbf865d1">Limites</h2>
<div class="outline-text-2" id="text-orgbf865d1">
<p>
Ne sont pas supportés:
</p>
<ul class="org-ul">
<li>Les modules et les foncteurs</li>
<li>Les enregistrements</li>
<li>la POO d'OCaml</li>
<li>la FFI</li>
<li>le pattern matching imbriqué</li>
<li>les arguments optionnels</li>
<li>les structures mutables dans le cas général</li>
<li>Les try/with en cas d'exception lancée</li>
</ul>
</div>
</div>
<div id="outline-container-org8e0ad52" class="outline-2">
<h2 id="org8e0ad52">Typage</h2>
<div class="outline-text-2" id="text-org8e0ad52">
</div>
<div id="outline-container-org2d57247" class="outline-3">
<h3 id="org2d57247">Core RAML</h3>
<div class="outline-text-3" id="text-org2d57247">
<p>
Après simplification de l'AST OCaml, on se ramène à un langage simple, où les
termes sont sous forme <i>let-share-normale</i>, ou l'on a remplacé le plus de sous
expressions possibles par les variables créées par let ou share (voir syntaxe).
Pour garantir que le programme transformé rend compte de la consommation du
programme en entrée, on peut marquer des constructions comme <i>gratuites</i>
(meta-syntaxe). Les constructions gratuites sont ignorés dans l'analyse. On
présente ici un langage réduit, sans types primitifs comme les tableaux, leurs
opérations, ou conditionelle.
</p>
</div>
<div id="outline-container-org0707491" class="outline-4">
<h4 id="org0707491">Syntaxe</h4>
<div class="outline-text-4" id="text-org0707491">
<dl class="org-dl">
<dt>Les classiques</dt><dd>variables, application n-aire, abstraction, constructeurs,
tuples, références, let, let rec de fonctions, pattern matching</dd>
<dt>fail</dt><dd>pour modéliser les exceptions</dd>
<dt>tick(q ∈ ℚ)</dt><dd>consommation de ressources</dd>
<dt>share x as (y,z) in</dt><dd>partage de valeurs</dd>
</dl>
</div>
</div>
<div id="outline-container-org488c54d" class="outline-4">
<h4 id="org488c54d">Définitons préalables</h4>
<div class="outline-text-4" id="text-org488c54d">
<dl class="org-dl">
<dt>K ∈ Syn</dt><dd>les neouds de syntaxe</dd>
<dt>M : Syn × ℕ -&gt; ℚ</dt><dd>Une <i>métrique de ressource</i></dd>
<dt>x ∈ Var</dt><dd>les variables</dd>
<dt>l ∈ Loc</dt><dd>les adresses du tas</dd>
<dt>H : Loc -&gt; Val</dt><dd>Un tas, de support fini</dd>
<dt>V : Var -&gt; Loc</dt><dd>Un environnement, partiel</dd>
<dt>S = · | l : S</dt><dd>Une pile d'arguments</dd>
<dt>null ∈ dom(H)</dt><dd>Un pointeur nul tel que H(null) = null</dd>
</dl>
</div>
</div>
<div id="outline-container-orge949f48" class="outline-4">
<h4 id="orge949f48">Valeurs</h4>
<div class="outline-text-4" id="text-orge949f48">
<dl class="org-dl">
<dt>l</dt><dd>position d'une valeur sur le tas</dd>
<dt>(l1, &#x2026;, ln)</dt><dd>tuples de valeur</dd>
<dt>(λx.e, V)</dt><dd>Fermeture (avec son jus)</dd>
<dt>C l</dt><dd>Application de constructeur à une valeur</dd>
</dl>
</div>
</div>
<div id="outline-container-org023f391" class="outline-4">
<h4 id="org023f391">Sémantique à grand pas</h4>
<div class="outline-text-4" id="text-org023f391">
<p>
Le jugement de la sémantique est
</p>
<pre class="example">
S,V,H  ⊢_Μ e ⇓ w | (q,q')
</pre>
<p>
qui s'entend par: Dans un environnement V, avec une pile S et un tas H,
l'expression e se réduit en un nombre d'étape en w, en induisant un coût (q,q')
selon la métrique de ressource M. Les réductions w sont soit
</p>
<dl class="org-dl">
<dt>(l, H')</dt><dd>Une valeur de retour et un nouvel état</dd>
<dt>⊥</dt><dd>Echec (exceptions, accès de tableau invalide&#x2026;)</dd>
<dt>○</dt><dd>Non-terminaison</dd>
</dl>
</div>
</div>
</div>
<div id="outline-container-org7a052a2" class="outline-3">
<h3 id="org7a052a2">Consommation de ressource</h3>
<div class="outline-text-3" id="text-org7a052a2">
<p>
Les consommation de ressources sont de la forme (q,q') où q est la "marée
haute", la quantité maximale de ressource dont on à besoin, et q' est la quantité
de ressource disponible après l'évaluation. La consommation est paramétrée par
une <i>métrique de ressources</i> M, qui est une fonction qui à un noeud de la
syntaxe k et sa taille n associe un coût propre de la construction M(k,n) =
(q,q'). Les consommations forment un monoïde avec
</p>
<ul class="org-ul">
<li>0 = (0,0)</li>
<li>(q,q')·(p,p') =
<ul class="org-ul">
<li>(q + p - q', p') si q' ≤ p</li>
<li>(q, p' + q' - p) si q' &gt; p</li>
</ul></li>
</ul>
<p>
La composition de deux consommation est la consommation de la séquence.
Dans le cas d'une analyse de WCET, les consommation sont (q, 0) car on le libère
pas de temps. En général, on note la consommation (q, 0) par q et (0, -q) par
-q. On peut alors dénoter la consommation des structures de données, par
exemple:
</p>

<p>
(e1,e2) a une consommation M(tuple,2)·(q1,q1')·(q2,q2')
</p>
</div>
</div>
</div>
]]></description>
  <link>hectorsuzanne.comhoffmann2016.html</link>
  <pubDate>Lun, 06 avr 2020 00:00:00 +0200</pubDate>
</item>
<item>
  <title>Notes de lectures: Closed-form Upper Bounds in Static Cost Analysis</title>
  <description><![CDATA[<div id="outline-container-orgaee0df4" class="outline-2">
<h2 id="orgaee0df4">Introduction</h2>
<div class="outline-text-2" id="text-orgaee0df4">
<p>
On cherche à borner le coût en une certaine ressource d'un programme <i>P</i> sur une
entrée arbitraire <i>x</i> sans pour autant avoir à déterminer le <i>x</i> qui réalise
cette borne ou avoir à calculer <i>P(x)</i>. Pour ce faire, on procède avec un
programme <i>P</i> et un <i>modèle de coût</i> donné. On produit dans un premier temps des
<i>relations de coûts (CR)</i> pour <i>P</i>. Ces relations décrivent le coût selon le
modèle du calcul de <i>P</i> en fonction de la <i>taille</i> de son entrée <i>x</i>.
</p>

<p>
Les CR sont un formalisme pertinent pour l'analyse de coût pour les raisons
suivantes:
</p>
<ul class="org-ul">
<li>Elles sont agnostiques au langage et modèle de coût utilisés</li>
<li>Elles couvrent de nombreuses classes de complexités algorithmiques, et sont
donc polyvalentes</li>
<li>On peut y capturer des notions non-triviales de coûts, même à posteriori en
changeant le modèle.</li>
</ul>
<p>
La dynamique des CR est néanmoins non-triviale: l'abstraction des données du
programme à leurs tailles crée une source de non-déterminisme: par exemple,
quand on branche selon une valeur du programme, et que les deux branches n'ont
pas de coût égaux, notre analyse statique doit résoudre l'ambiguïté.
</p>

<p>
Dans la littérature, on utilise souvent des <i>relations de récurrences</i> à la
place des relations de coûts. Cela à conduit à essayer de déléguer la recherche
de formes fermées des relations de coûts par des systèmes experts de calculs
symboliques (Mapple, Mathematica,etc.). Mais, nous le verrons, cette approche na
pas eue la succès escompté.
</p>

<p>
Notre nouvelle approche, basée sur l'analyse statique d'une sémantique de
programmation par contrainte des CR, est la première permettant d'obtenir des
formes closes des CR automatiquement pour un grand nombre de programmes
impératifs représentatifs de ceux présent dans le monde réel.
</p>
</div>
</div>

<div id="outline-container-org6a5c484" class="outline-2">
<h2 id="org6a5c484">Relations de Coûts</h2>
<div class="outline-text-2" id="text-org6a5c484">
</div>
<div id="outline-container-org315f9d6" class="outline-3">
<h3 id="org315f9d6">Obtention</h3>
<div class="outline-text-3" id="text-org315f9d6">
<p>
On veut pouvoir inférer, depuis le code source d'un programme impératif
structuré (avec <code>if</code>, <code>while</code>,&#x2026;) ou non-structuré (avec <code>goto</code>, <code>jump</code>) les
relations de tailles qui existent entre les données qu'il manipule. On procède
en trois étapes:
</p>
<ol class="org-ol">
<li>On construit les <i>Control Flow Graph</i> du programme pour rendre les
récurrences explicites.</li>
<li>En utilisant l'interprétation abstraite, on obtient des <i>relation de tailles</i>
entres les différents appels récursifs. Les relations obtenues sont
généralement linéaires (i.e. on travaille dans les domaine abstrait des
polyèdres convexes), mais on peut utiliser un autre formalisme à condition
d'avoir un solveur adapté plus loin dans le traitement.</li>
<li>On interprète le programme dans le modèle de coût choisi, afin de générer les
relations de coûts en elles-mêmes.</li>
</ol>
</div>
</div>

<div id="outline-container-orgabfeade" class="outline-3">
<h3 id="orgabfeade">Syntaxe</h3>
<div class="outline-text-3" id="text-orgabfeade">
<pre class="example">
r   ::= ℝ+
n   ::= ℕ
v   ::= Ζ
lin ::= v1*x1 + ... vn*xn
c   ::= lin &lt; lin | lin ≤ lin | lin = lin
φ   ::= c+

exp ::= r         | nat(lin)
      | exp + exp | exp * exp  | exp - r
      | exp ^ r   | log_n(exp) | n^exp
      | max( exp+ )

cost ::=  C(x1...xn) = exp + ∑(i=1...n) D_i(yi1...yim) where φ
CRS  ::= cost+
</pre>

<p>
On notera que dans <code>cost</code>, les variables de <code>exp</code> et <code>y1..ym</code> ne sont pas
<i>définies</i> grâce à <code>x1...xn</code>, mais seulement reliées à elles par les contraintes
présentes dans <code>φ</code>.
</p>
</div>
</div>

<div id="outline-container-org9a7af06" class="outline-3">
<h3 id="org9a7af06">Sémantique</h3>
<div class="outline-text-3" id="text-org9a7af06">
<p>
La sémantique des <i>CRS</i> suit celle d'un programme en progammation logique par
contrainte: Pour évaluer un <code>C(v1...vn)</code>, on procède par étape:
</p>
<ol class="org-ol">
<li>On choisie une des clauses du CRS qui a <code>C</code> en tête.</li>
<li>on instancie les variables <code>x1...xn, yi1...yim, vars(exp)</code> de manière à
satisfaire <code>φ</code>.</li>
<li>On calcule et accumule <code>exp</code> sous l'instanciation précédente</li>
<li>On itère sur tout les <code>D_i</code></li>
</ol>
<p>
Quand on ne peut pas satisfaire <code>φ</code>, on a atteint un échec.
La sémantique est parallèle à celle d'un programme Prolog.
</p>

<p>
On définit alors la sémantique d'un CRS comme l'ensemble de arbres d'évaluations
qu'il engendre. Le coût d'un arbre est défini par induction, et le coût du CRS
l'ensemble des coût que ses arbres d'évaluation.
</p>

<p>
On note <code>Trees(C(v1...vn), S)</code> l'ensemble des arbres de l'évaluation de
<code>C(v1...vn)</code> dans le CRS <code>S</code>, et <code>Answer(C(v1...vn), S)</code> l'ensemble des coûts
engendrés.
</p>
</div>
</div>

<div id="outline-container-org7bf2e0c" class="outline-3">
<h3 id="org7bf2e0c">Comparaison avec des RR</h3>
<div class="outline-text-3" id="text-org7bf2e0c">
<dl class="org-dl">
<dt>Non-déterminisme</dt><dd>L'abstraction des valeurs par leurs tailles rends les CR
<i>très</i> non-déterministes, même dans les cas ou le programme ne l'est pas. Voir
par exemple la fonction <code>filter</code>.</dd>
<dt>Contraintes inexactes</dt><dd>Les arguments des appels récursifs des CR ne sont
liées aux arguments à la racine que par des relations imprécises. On peut
avoir à les tirer dans les ensembles infinis. Dans les RR, les arguments des
appels récursifs sont connus si on connaît les arguments de l'appel précédent.</dd>
<dt>Arguments multiples</dt><dd><p>
Dans les RR, l'argument décroît selon un ordre bien
fondé, ce qui garantie la terminaison: on parle de récurrence structurelle.
Dans les CR, l'ordre bien-fondé peut exister sur une <i>relation de taille</i>
arbitraire.
</p>

<p>
Les deux premiers cas sont une source importante de non-déterminisme du
résultat. Les CR ne définissent pas des fonctions mais des relations entre les
tailles des arguments et la "taille" du calcul associé. De plus, les systèmes
experts de résolutions de récurrence ne peuvent généralement par inférer seuls
l'argument décroissant de manière bien-fondé nécessaire à la résolution de la
récurrence. Ces systèmes ne sont pas adaptés à la mise en forme close de CR.
</p>

<p>
On pourrait, enfin essayer de simplifier les CR pour en retirer le
non-déterminisme. Hélas, les exemples réels simples montrent sans ambiguïté
qu'il existe des CR non-déterministes dont la borne sup n'est pas modélisable
par celle d'une de ses sous-CR déterministe.
</p></dd>
</dl>
</div>
</div>
</div>

<div id="outline-container-orgee1b3e8" class="outline-2">
<h2 id="orgee1b3e8">Bornes Supérieures de Forme Fermées des Relations de Coûts</h2>
<div class="outline-text-2" id="text-orgee1b3e8">
<p>
On définit les <i>Bornes Supérieures de <code>C</code> de formes fermées dans un CRS</i> <code>S</code>
comme une fonction <code>f : Ζ^n -&gt; ℝ+</code> telle que:
</p>
<ul class="org-ul">
<li><code>f(x1...xn) = exp</code></li>
<li><code>∀ v1...vn, ∀r ∈ Answers(C(v1...vn), S), f(v1...vn) ≥ r</code></li>
</ul>

<p>
La méthode décrite dans l'article construit les bornes sup <code>f</code> en approchant par
le haut le nombre et le coût individuel des noeuds internes et des feuilles des
arbres d'évaluation de <code>C</code>. Formellement:
</p>

<p>
<code>f(x) = internes(x)*cout-interne(x) + feuilles(x)*cout-feuille(x)</code>
</p>
</div>

<div id="outline-container-orga1b9cd4" class="outline-3">
<h3 id="orga1b9cd4">Bornage du Nombre de Noeuds</h3>
<div class="outline-text-3" id="text-orga1b9cd4">
<p>
On borne les valeurs de <code>internes(x)</code> et <code>feuilles(x)</code> en donnant une borne supérieure
de la hauteur <code>h(x)</code> et du <i>facteur de branchement</i> <code>b</code> des arbres d'évaluation.
Le facteur de branchement est facilement borné par le nombre maximal d'appels
récursifs dans une équation pour <code>C</code>. Il est donc immédiatement calculable.
</p>

<p>
On cherche maintenant à borner, pour un arbre d'évaluation <code>T ∈
Trees(C(v1...vn), S)</code>. On applique un pré-traitement à <code>S</code> pour garantir un
invariant sur <code>T</code>: C'est la <i>Mise en forme de récurrence directe</i>: Dans une
descente dans <code>T</code>, les noeuds successifs représentent des appels imbriqués aux
relations de coût idoines, étiquetés par les têtes des relations (<code>C</code>, <code>D</code>,
&#x2026;); La forme de récurrence directe impose que si <code>C</code> apparaît comme un
descendant de <code>C</code>, alors c'est un descendant direct. Ce résultat permet de
réduire le bornage de <code>h(x)</code> au bornage du nombre d'appels successifs à une
relation de coût <code>C</code>. Une technique de passage en forme de récurrence directe
est présentée plus loin.
</p>

<p>
Le bornage du nombre d'appel successifs à une relation de coût ou de récurrence
à été étudié dans le cadre de l'analyse de terminaison, aussi il existe des
algorithmes d'inférence de <i>fonctions de classement</i>, qui associe aux arguments
des relations un <i>rang</i> dans un ordre bien-fondé. Si une fonction de classement
<code>f_C</code> existe pour une relation de coût <code>C</code>, alors la hauteur de l'arbre engendré
par <code>C(v1...vn)</code> est bornée <code>f_C(v1...vn)</code>. Les fonctions de classements que
nous utilisons sont linéaires, et inférées par l'algorithme décrit par Podelski
&amp; Rybalchenko dans <i>A complete method for the for synthesis of linear ranking
functions (VMCAI04)</i>.
</p>

<p>
Une fois les bornes de la hauteur est du facteur de branchement établie, il
suffit d'approximer l'arbre d'évaluation par un arbre complet de hauteur
<code>h(v1...vn)</code> et de facteur <code>b</code>.
</p>
</div>
</div>

<div id="outline-container-org3b3ded6" class="outline-3">
<h3 id="org3b3ded6">Bornage du Coût par Noeud</h3>
<div class="outline-text-3" id="text-org3b3ded6">
<p>
Il reste maintenant à obtenir les fonctions <code>cout-interne</code> et <code>cout-feuille</code>.
Premièrement, on utilise l'interprétation abstraite sur la forme de récurrence
directe pour obtenir des approximations sûres des invariants entre les appels
successifs aux relations de coûts. Les coûts sont monotones en leurs composants
<code>nat</code>. Ils suffit alors de borner ces composants en utilisant les invariants
calculés plus haut et les arguments à la racine de l'arbre d'évaluation. (Voir
la functions <code>ub_exp</code> de l'article original pour le code exact). Les relations
de coûts récursives de <code>S</code> engendrent les noeuds internes, et les non-récursives
engendrent les feuilles. Les bornes supérieures calculées ici forment les
fonctions <code>cout-interne</code> et <code>cout-feuille</code>.
</p>
</div>
</div>

<div id="outline-container-orgb4711bc" class="outline-3">
<h3 id="orgb4711bc">Cas des Algorithmes <i>Diviser-pour-régner</i></h3>
<div class="outline-text-3" id="text-orgb4711bc">
<p>
Dans les cas des algorithmes diviser-pour-régner, l'approximation
noeud-par-noeud n'est pas assez précise. On borne alors le coût d'une relation
de coût <code>C</code> par <code>C+(x) = levels(x)*cout-level(x)</code>. Comme précédemment, on borne
de nombre de niveau avec la hauteur de l'arbre.
</p>

<p>
Pour borner le coût par niveau, nous développons une caractérisation des CR
<i>diviser-pour-régner</i> compatible avec notre analyse, un test automatique
d'appartenance à cette classe de CR, et une méthode pour calculer une borne
<code>cout-level</code> pour ces CR.
</p>

<dl class="org-dl">
<dt>Caractérisation</dt><dd>Une CR est <i>diviser-pour-régner</i> si, pour tout ses arbres
d'évaluation, le coût propre d'un niveau de son arbre d'évaluation des
supérieur ou égal au coût propre du niveau directement en dessous</dd>
<dt>Calcul</dt><dd>On calcule, pour une CR <code>C</code>, les paires des coûts abstraits propres
de chaque équation définissant <code>C</code> et de la somme des coût abstraits propres
de leurs enfants respectif. Pour prouver que <code>C</code> est diviser-pour-régner, il
suffit de prouver que pour tout assignement satisfaisant les contraintes
locales à ces noeuds, le coût de <code>C</code> est au moins égal au coût total de ces
enfants.</dd>
<dt>Bornage</dt><dd>On borne alors le coût d'un niveau par le coût de la racine, qui
est le niveau de coût maximal. L'algorithme de calcul est le même que pour le
calcul de <code>cout-interne</code>.</dd>
</dl>
</div>
</div>
</div>

<div id="outline-container-org75b78cf" class="outline-2">
<h2 id="org75b78cf">Mise en Forme de Récurrence Directe des Relations de Coûts</h2>
<div class="outline-text-2" id="text-org75b78cf">
<p>
<b>TODO</b> On a rien compris à leurs définition maître de "BTC". En même temps ils
pourrait la définir proprement. Mgr. Gigard aurait eu des mots durs sur cette
partie.
</p>

<p>
Mais, en gros, il s'agit de déterminer, par une analyse de composante fortement
connexe dans le CFG des système de CR, lesquelles sont définies récursivement et
le cas échéant d'effectuer assez l'inlining pour rendre toutes les définitions
récursives <i>directement</i> récursives.
</p>
</div>
</div>

<div id="outline-container-org8e5a80c" class="outline-2">
<h2 id="org8e5a80c">Incomplétude de l'Analyse de Coût</h2>
<div class="outline-text-2" id="text-org8e5a80c">
<p>
La terminaison de programme peut se réduire à un calcul de coût dans un modèle.
Donc notre analyse sera forcément incomplète. Même quand un programme admet un
coût fini dans le modèle considéré, notre analyse comporte des sources de pertes
de précision:
</p>
<ul class="org-ul">
<li>La transformation du programme en système de relations de coût utilise des
techniques d'interprétation abstraites qui donnent des résultats approximatifs
sur des problèmes comme l'aliasing ou les relations de tailles. On notera que
certain de ces problèmes sont en soit indécidables.</li>
<li>L'obtention de bornes supérieures pour les systèmes de CR est aussi
indécidable, même pour des systèmes appauvris. On distingue les pertes de
précisions suivantes dans notes analyse :
<ul class="org-ul">
<li>La mise en forme de récurrence directe n'est pas systématiquement faisable,
notamment dans le cas de définition mutuellement récursives.</li>
<li>Certaines CR n'ont pas de fonction de classement linéaires. On pourrait
étendre nos fonctions de classement à des classes plus expressives, mais le
besoin ne s'est pas fait ressentir en pratique.</li>
<li>Les invariants recherchés lors du bornage du coût par noeud peuvent ne pas
être linéaires. Pour intégrer des classes d'invariants plus intéressantes,
il conviendrait d'adapter la procédure de maximisation.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org5db672b" class="outline-2">
<h2 id="org5db672b">Évaluation Expérimentale</h2>
<div class="outline-text-2" id="text-org5db672b">
<p>
<b>TODO</b>
</p>
</div>
</div>

<div id="outline-container-org941b7e6" class="outline-2">
<h2 id="org941b7e6">Travaux Connexes &amp; Conclusion</h2>
<div class="outline-text-2" id="text-org941b7e6">
<p>
Dès le système <i>METRIC</i> de Wegbriet, on cherche à obtenir des CR à partir de
programme. Mais <i>METRIC</i> se limite aux RR, qui sont déterministes, et déjà, les
solution exactes sous formes closes sont ors de portée. On se contente alors de
bornes supérieures. On retrouve ensuite les travaux de Métayer avec <i>ACE</i>, de
Rosenthal avec l'interprétation abstraite, de Walder avec l'analyse de rigueur
au service de l'analyse de ressources, et Sands et ses théories d'équivalence de
coûts. Notre avancée consiste à considérer des relations (1) non-déterministes,
(2) augmentées de contraintes de tailles sur leurs arguments.
</p>

<p>
Les systèmes <b>algébriques</b> sont basés sur les travaux extensifs sur la
résolution de relations de récurrences, soit en implémentant un solveur limité
en tant que composant de l'analyseur, soit en déléguant la résolution à un
<i>Computer Algebra System</i>. On citera <i>Mathematica, Mapple, Maxima</i>&#x2026;
</p>

<p>
Les systèmes <b>transformationnels</b> représentent les CR comme des programmes
fonctionnels, et la mise en forme close correspond à une réécriture du programme
de base augmenté d'une notion de consommation de ressource. <i>ACE</i> fût le premier
de ces systéme, basé sur une grande quantité de règles de réécriture manuelles.
Depuis, des avancées ont permis de trouver des bornes sous forme close pour
certaines classes de programmes (cf. Rosenthal FPCA'89).
</p>

<p>
Les deux approches souffrent de problèmes en cas de non-déterminisme des CR, ce
qui est assez courant dans le monde réel, et les résultats obtenus sont rarement
utiles s'il sont trop coûteux ou trop précis, et donc algébriquement complexes
(que faire de 5*sqrt(5)*(1-sqrt(5))<sup>(x+1)</sup> - 5*sqrt(5)*(1+sqrt(5))<sup>(x+1)</sup> + &#x2026; ?)
Au contraire, l'approche décrite dans cette article produit des résultats
corrects et simple même en cas de non-déterminisme.
</p>

<p>
On note l'existence de <i>PURRS</i>, qui ressemble à notre travail, mais avec
uniquement des CR déterministes, et des travaux de Marion&amp;al. sur les bornes
polynomiales des tailles de piles pour les programmes fonctionnels.
</p>

<p>
Notre travail mets en lumière la pertinence des CR comme <i>langage cible</i> des
analyses de coûts pour une analyse agnostique du langage source. Nous avons
étendu notre travail à des CR prenant en compte les phénomènes de libération
mémoire par un ramasse-miette, et donc le bornage de tas.
</p>
</div>
</div>
]]></description>
  <link>hectorsuzanne.comalbert2008.html</link>
  <pubDate>Lun, 06 avr 2020 00:00:00 +0200</pubDate>
</item>
</channel>
</rss>
